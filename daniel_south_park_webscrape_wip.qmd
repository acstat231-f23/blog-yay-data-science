```{r}
library(tidyverse)
library(rvest)
library(robotstxt)
library(purrr)
library(stringr)
library(dplyr)
library(tidytext)
library(kableExtra)
```

```{r}
#|eval: false

#testing
south_park <- "https://southpark.fandom.com/wiki/Freemium_Isn%27t_Free/Script"

#IT WORKS
robotstxt::paths_allowed(south_park)

#test from an episode
sp_18_06 <- south_park %>% #dataframe name will be like temp2           
  read_html() %>%
  html_elements("table") %>%
  purrr::pluck(2) %>% #the table is always 2 for scripts
  html_table() %>%
  mutate(Episode_Title = "Freemium Isn't Free") #pick from sp_title_raw
  

#this stuff can be after the full dataframe is made?
colnames(sp_18_06) <- c('Character', 'Line', 'Title') #again, temp2

filter(sp_18_06, Character == "Kenny") #this isn't working

#remove any strings contained within []
#anything with empty observations on character should be removed
#also any observation with the title on character variable should be removed



#to generalize this code, here's some logic that could work
#using the general link + the link name, do this code above. store this information into a temporary variable
#do something similar to a for loop. add this temp variable to the full_south_park_database (not made yet) using merge
#make sure you have character, their line, and the episode name

```

```{r}
#|eval: false

south_park_episodes <- "https://en.wikipedia.org/wiki/List_of_South_Park_episodes" 

robotstxt::paths_allowed(south_park_episodes)

#blank dataframe that has the variables from web scraping
sp_title_raw <- data.frame(matrix(ncol = 3, nrow = 0))

#provide column names
colnames(sp_title_raw) <- c('Title')

#loop all 26 seasons
for (i in 2:27) {
  temp <- south_park_episodes %>%
  read_html() %>%
  html_elements("table") %>%
  purrr::pluck(i) %>% #up to 27
  html_table() %>% #now join all this mumbo jumbo with sp_title_raw
  select(Title)
  
  if (i == 2) {
    sp_title_raw <- temp
  } else {
  sp_title_raw <- merge(x=sp_title_raw,y=temp, all=TRUE)
  }
}

#renaming the titles
sp_title_raw <- sp_title_raw %>%
  mutate(Title = str_replace_all(Title, "#", "")) %>%
  mutate(Title = str_replace_all(Title, "\"", ""))

#create two columns: one for actual title and one for link title
sp_title_raw <- sp_title_raw %>%
  mutate(Link_Title = Title,
         Link_Title = str_replace_all(Link_Title, " ", "_"),
         Link_Title = str_replace_all(Link_Title, "\'", "%27"),
         Link_Title = str_replace_all(Link_Title, "\\?", "%3F"),
         Link_Title = str_replace_all(Link_Title, "Ã®", "i")
         )
  #some scripts are not accounted for (eg. episodes with two names: World Wide Recorder ConcertThe Brown Noise[100]). Since this is only a few observations, it is fine to not worry about it too much

sp_title_raw <- sp_title_raw %>%
  mutate(URL = paste("https://southpark.fandom.com/wiki/",
                     sp_title_raw$Link_Title, "/Script", sep=""))
  
save(sp_title_raw, file="sp_title_raw.Rdata")
```

```{r}
#|eval: false

#test on scraping for one episode
load(file="sp_title_raw.Rdata")

#dataset ordered not based on season... doesn't really matter for our purposes
first_url <- sp_title_raw$URL[1]

sp_first <- first_url %>%
  read_html() %>%
  html_elements("table") %>%
  purrr::pluck(2) %>% #the table is always 2 for scripts
  html_table()

#quick wrangling code to think about (within the loop)
sp_first <- sp_first %>%
  mutate(Title = sp_title_raw$Title[1]) %>% #pick from sp_title_raw
  slice(-1, -n()) #remove first and last observation (not lines)

#remove when generalizing
sp_complete <- sp_first 
  
#further wrangling code (outside the loop)
colnames(sp_complete) <- c('Character', 'Line', 'Title')

#finish the full wrangling... (I need help)
sp_complete <- sp_complete %>%
  filter(!is.na(Character)) #THIS ISN'T WORKING
  #remove any words that are contained in []


#we have created a base for our loop (for all 321 available episodes)
```

```{r}
#|eval: false
#for all scripts

sp_scripts_full <- sp_title_raw$URL[1] %>%
          read_html() %>%
          html_elements("table") %>%
          purrr::pluck(2) %>% #the table is always 2 for scripts
          html_table() %>%
          mutate(Title = sp_title_raw$Title[j]) %>%
          slice(-1, -n()) 

for (j in 2:321) {
  
  skip_to_next <- FALSE
  
  tryCatch(
    error = function(cnd) {
      skip_to_next <- TRUE
    },
 
    if (skip_to_next) { next }  
    else {
         sp_scripts_full <- sp_title_raw$URL[j] %>%
            read_html() %>%
            html_elements("table") %>%
            purrr::pluck(2) %>% #the table is always 2 for scripts
            html_table() %>%
            mutate(Title = sp_title_raw$Title[j]) %>%
            slice(-1, -n()) |>
            bind_rows(sp_scripts_full)
    }
  )
}

save(sp_scripts_full, file="sp_scripts_full.Rdata")
```

```{r}
#|eval: false
#wrangling the full dataset
load(file="sp_scripts_full.Rdata")


colnames(sp_scripts_full) <- c('Character', 
                               'Line', 
                               'Episode', 
                               'Empty')

sp_scripts_full <- sp_scripts_full %>%
  filter(!Character == "" & !Line == "" & !Episode == "") %>%
  select(c('Character', 'Line', 'Episode')) %>%
  mutate(Line = gsub("\\[.*?\\]", "", Line))
  


sp_lines_full <- sp_scripts_full %>%
  unnest_tokens(output = word, input = Line) #make sure filter command is working above

#str_which(sp_lines_full$word, "jew") |> length()
#this concerns me because I swear they say jew way more in the show... 

sp_lines_full_ngram <- sp_scripts_full %>%
  unnest_tokens(output = threegram, input = Line, token = "ngrams", n = 3)
  
```

```{r}
#number of unique episodes that I have
length(unique(sp_scripts_full$Episode))
#proportionally, we have 88.7% of all south park episodes (not movies, since our analysis is on the shows)
```

```{r}
#|eval: false
#remove the stop words!
data(stop_words)

custom_stop_words <- data.frame(
  word = c("yeah", 
           "gonna", 
           "hey", 
           "uh", 
           "wanna", 
           "alright", 
           "huh", 
           "gotta",
           "butters",
           "kyle",
           "stan",
           "cartman",
           "kenny",
           "eric",
           "ah"), 
  lexicon = c("custom", "custom", "custom", "custom", 
              "custom", "custom", "custom", "custom",
              "custom", "custom", "custom", "custom",
              "custom", "custom", "custom")
)

stop_words_full <- bind_rows(stop_words, custom_stop_words)

stop_words_full %>% 
  count(lexicon) #add stop words here (eg. um). can remove too

sp_lines_filtered <- sp_lines_full %>%
  anti_join(stop_words_full, by="word")

save(sp_lines_filtered, file="sp_lines_filtered.Rdata")

  #note that some words in the script are not accounted for (eg. Ky-yle =/= kyle)
```


```{r}
load(file="sp_lines_filtered.Rdata")

#sentiment analysis: words like FUCK (make sure it's not case sensitive)
#then, we compare between other people in the group
```

```{r}
sp_lines_filtered |>
  count(word, sort = TRUE) |>
  slice(1:35) |>
  # fct_reorder is used to re-order the axis (displaying the word) 
  # by values of n (the number of times that word was used)
  ggplot(aes(x = fct_reorder(word, n), 
             y = n, 
             color = word, 
             fill = word)) +
  geom_col() +
  # Rotate graph
  coord_flip() +
  guides(color = "none", 
         fill = "none") +
  labs(
    # Remove x variable label; notice that although coordinates are flipped, 
    # the labels correspond to which variables were specified 
    # as `x` and `y` in `aes()`
    x = NULL,
    y = "Number of instances",
    title = "Most Common Words for South Park (Excl. Stop Words")
```

```{r}
sp_lines_full_ngram |>
  count(threegram, sort = TRUE) |>
  slice(2:36) |> #NA for 1, annoying
  ggplot(aes(x = fct_reorder(threegram, n), 
             y = n, 
             color = threegram, 
             fill = threegram)) +
  geom_col() +
  coord_flip() +
  guides(color = "none", 
         fill = "none") +
  labs(
    x = NULL,
    y = "Number of instances",
    title = "Most Common ThreeGrams for South Park")
```

```{r}
#sentiments (positive and negative) color coded (graph above)
pos_neg_val_sentiment <- get_sentiments("afinn") #from tidyverse

#then mutate filtered words with the pos/neg sentiments associated with the words
sp_lines_filtered_pos_neg <- full_join(sp_lines_filtered, 
                                          pos_neg_val_sentiment, 
                                          by="word")

#now remove any lines where sentiment is NA
sp_lines_filtered_pos_neg <- sp_lines_filtered_pos_neg %>%
  filter(!is.na(value))

#now we can do something similar with descriptive sentiments (eg. hell feels angry)
word_sentiment <- get_sentiments("nrc") 
sp_lines_filtered_sentiments <- full_join(sp_lines_filtered, 
                                          word_sentiment, 
                                          by="word")
sp_lines_filtered_sentiments <- sp_lines_filtered_sentiments %>%
  filter(!is.na(sentiment))
```
```{r}
#ANALYSIS 1

#in these quick analyses, find the overall sentiment of the show (score) and the most frequent words that come up when describing the words used in the show (word_sentiment). One word can have more than one sentiment

sp_lines_filtered_sentiments|>
  count(sentiment, sort = TRUE) |>
  slice(1:10) |> #NA for 1, annoying
  ggplot(aes(x = fct_reorder(sentiment, n), 
             y = n, 
             color = sentiment, #maybe important... bad words red, good words green?
             fill = sentiment)) +
  geom_col() +
  coord_flip() +
  guides(color = "none", 
         fill = "none") +
  labs(
    x = NULL,
    y = "Number of instances",
    title = "Most Common Sentiments in South Park")

mean(sp_lines_filtered_pos_neg$value)
```

```{r}
#bare code for counting IMPORTANT words spoken by a character and ALL words

character <- c("Cartman",
               "Kenny",
               "Kyle",
               "Stan")

filtered <- c(sum(sp_lines_filtered$Character == "Cartman"),
              sum(sp_lines_filtered$Character == "Kenny"),
              sum(sp_lines_filtered$Character == "Kyle"),
              sum(sp_lines_filtered$Character == "Stan"))

unfiltered <- c(sum(sp_lines_full$Character == "Cartman"),
              sum(sp_lines_full$Character == "Kenny"),
              sum(sp_lines_full$Character == "Kyle"),
              sum(sp_lines_full$Character == "Stan"))

sp_characters <- data.frame(character, filtered, unfiltered)

sp_characters <- sp_characters %>%
  mutate(filtered_percent = 100*(filtered / as.integer(count(sp_lines_filtered)))) %>%
  mutate(unfiltered_percent = 100*(unfiltered / as.integer(count(sp_lines_full))))

sp_characters %>%
  kable(booktabs = TRUE, digits = 2, col.names = c(
    "Character",
    "Filtered Words",
    "Unfiltered Words",
    "% of all Filtered words",
    "% of all Unfiltered words"
  )) %>%
  kable_styling()

#kenny definitely looks much lower than expected... not sure why, maybe wrangling error?
```


```{r}
#lastly to combine all of this, find the sentiments of each main character
#aka repeat analysis 1 for each main character (2)
#faceting code needed




#group_by(character)
#then filter by the characters I want
#facet the graphs



#okay this looks very low...
sp_lines_filtered |>
  filter(Character == c("Cartman", "Kenny", "Kyle", "Stan")) |>
  group_by(Character) |>
  count(word, sort = TRUE) |>
  slice(1:10) |>
  ggplot(aes(x = fct_reorder(word, n),
             y = n,
             color = word,
             fill = word)) +
  geom_col() +
  coord_flip() +
  guides(color = "none",
         fill = "none") +
  labs(
    x = NULL,
    y = "Number of instances",
    title = "Most Common Words for South Park (Excl. Stop Words")

#kable again to create a nice sentiment value chart between these characters
#mean(sp_lines_filtered_pos_neg$value)


```

```{r}
#code for finding average sentiment of the 4 main characters
#filter for Character == "Cartman", then find the average of the values

#get the means of sp_lines_filtered_pos_neg$value by each character

avg_sentiment <- aggregate(mean(sp_lines_filtered_pos_neg$value),
                           by = list(c("Cartman", "Kenny", "Kyle", "Stan"),
                           FUN = mean))
  

sp_characters_pos_neg <- data.frame(character, avg_sentiment)


sp_characters_pos_neg %>%
  kable(booktabs = TRUE, digits = 2, col.names = c(
    "Character",
    "Avg. Sentiment"
  )) %>%
  kable_styling()
```

```{r}
sp_lines_filtered |>
  filter(Character == "Cartman") |> # to display a specific character's vocabulary
  count(word, sort = TRUE) |>
  slice(1:20) |>
  ggplot(aes(x = fct_reorder(word, n), y = n, color = word, fill = word)) +
  geom_col() +
  # Rotate graph
  coord_flip() +
  guides(color = "none", 
         fill = "none") +
  labs(
    # Remove x variable label; notice that although coordinates are flipped, 
    # the labels correspond to which variables were specified 
    # as `x` and `y` in `aes()`
    x = NULL,
    y = "Number of instances",
    title = "The most common words said by Cartman in South Park")
```
```{r}
sp_lines_filtered |>
  filter(Character == "Kenny") |> # to display a specific character's vocabulary
  count(word, sort = TRUE) |>
  slice(1:20) |>
  ggplot(aes(x = fct_reorder(word, n), y = n, color = word, fill = word)) +
  geom_col() +
  # Rotate graph
  coord_flip() +
  guides(color = "none", 
         fill = "none") +
  labs(
    # Remove x variable label; notice that although coordinates are flipped, 
    # the labels correspond to which variables were specified 
    # as `x` and `y` in `aes()`
    x = NULL,
    y = "Number of instances",
    title = "The most common words said by Kenny in South Park")
```

```{r}
sp_lines_filtered |>
  filter(Character == "Stan") |> # to display a specific character's vocabulary
  count(word, sort = TRUE) |>
  slice(1:20) |>
  ggplot(aes(x = fct_reorder(word, n), y = n, color = word, fill = word)) +
  geom_col() +
  # Rotate graph
  coord_flip() +
  guides(color = "none", 
         fill = "none") +
  labs(
    # Remove x variable label; notice that although coordinates are flipped, 
    # the labels correspond to which variables were specified 
    # as `x` and `y` in `aes()`
    x = NULL,
    y = "Number of instances",
    title = "The most common words said by Stan in South Park")
```

```{r}
sp_lines_filtered |>
  filter(Character == "Kyle") |> # to display a specific character's vocabulary
  count(word, sort = TRUE) |>
  slice(1:20) |>
  ggplot(aes(x = fct_reorder(word, n), y = n, color = word, fill = word)) +
  geom_col() +
  # Rotate graph
  coord_flip() +
  guides(color = "none", 
         fill = "none") +
  labs(
    # Remove x variable label; notice that although coordinates are flipped, 
    # the labels correspond to which variables were specified 
    # as `x` and `y` in `aes()`
    x = NULL,
    y = "Number of instances",
    title = "The most common words said by Kyle in South Park")
```

```{r}
sp_lines_filtered_pos_neg %>%
  filter(Character == c("Cartman", "Kenny", "Kyle", "Stan")) %>%
  group_by(Character)
  mean(sp_lines_filtered_pos_neg$value)
```

